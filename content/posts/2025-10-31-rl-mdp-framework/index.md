+++
date = '2025-10-23T09:39:41+08:00'
title = '強化式學習：MDP 框架'
tags = ['ml', 'read', 'zh']
draft = true
+++

## Why Reinforcement Learning？

大約兩個多月前，我在[強化式學習雜記]({{< ref "posts/2025-09-10-reinforcement-learning/index.md">}})一文中紀錄了我的一些想法，但沒有提到我選擇這個題目的動機：**除了學習新知本身的樂趣以外，我想要擴展我原本對機器學習的認識以包含強化式學習的框架，多一種的看待問題的角度**。

#### 監督式學習的框架

廣義來說，模型是人類對各種複雜現象的本質嘗試理解與簡化，提供了我們得以思考、評估、甚至量化並做出預測幫助解決問題的框架。

包含深度學習的回歸和分類器在內的監督式機器學習成果告訴我們，只要問題拆解後能夠以
$$ X \rightarrow y $$ 
的輸入輸出配對方式表達，並且取得足量的有效資料，那我們就有機會訓練出具一定泛化能力的擬合模型。即使不同的情境及資料型態會有各自的限制以及眉角，但核心概念都是從資料中學習由輸入預測輸出，找到一個良好的映射函數。

#### 強化式學習的世界

強化式學習試圖處理的問題是「代理人如何透過與環境的互動和回饋的引導學習出好的行動策略」。

對人類的日常經驗來說，依照回饋來修正自己行爲並不陌生，強化式學習乍看之下也和監督式學習頗爲相似，然而在一連串的互動和偶爾得到獎勵的情境下，學習該如何進行？要拿什麼作為輸入？又該如何定義答案？稍微思考後便會發現這過程並不簡單，也無法用函數擬合的方式直接想像。

原因無他，強化式學習問題本身就是一個更複雜的情境，因此需要額外的模型框架來描述，而這個關鍵的模型就是**馬可夫決策過程 Markov Decision Process（MDP）**。

MDP 為問題框架建立了形式化的數學框架，以此為基礎我們得以進一步
- 定義**期望折扣報酬 Expected Discounted Return** 以作為學習目標
- 理解**狀態 state value 價值、行動價值 state-action value** 的意義和值學習方法
- 以**貝爾曼期望方程式 Bellman equation** 為基礎，進行**時序差分學習 Temporal Difference Learning**

在問題的最後所需要的深度學習計算依舊能夠以函數擬合來思考，但要理解強化式學習的整體框架，還需要許多觀念作為階梯。

本文紀錄了我個人學習過程中的理解。數學部分來自經典的強化式學習課本 Sutton & Barto Reinforcement Learning - An Introduction。

---

## 1. 環境與代理人

{{< mermaid >}}
graph LR
    A[Agent] -- Action --> E[Environment] 
    E -- Reward --> A
    E -- Observation --> A
{{< /mermaid >}}

在強化式學習的問題中，首先要區別「環境（Environment）」與「代理人（Agent）」並了解兩者如何互動。代理人是學習的主角，環境則是代理人所在的世界或是系統，決定了代理人能取得的外在資訊。以遊戲的情境來說代理人就是玩家或者是遊戲 AI，環境則是遊戲本身。

#### 兩者之間的互動介面包含三種資訊
1. 觀察（Observation）又稱狀態（State）
2. 行動（Action）
3. 獎勵（Reward）又稱回饋

代理人對環境的了解來自於觀察和獎勵，而環境會根據其機制以根據代理人的行動決定新的狀態並提供對應的回饋。根據不同問題的假設，代理人不一定會掌握環境的知識，實際實環境的動態很像某種黑盒子。就好比駕駛人會知道自己踩下油門並看到汽車加速前進以及是否出車禍，但並不會知道汽車是如何運作的。

#### 獎勵作為目標的信號

當你漫無目的上路，觀察到的狀態景色會隨著你不同的行動而改變，但你選擇的路徑並沒有所謂好壞可言，因為沒有任何衡量基準在。當你的目標是「盡快回家」時，就產生了最短路線和繞遠路的差別；又若你的目標是「平安到家」，則安全駕駛就會優於搶快超車。

「獎勵」在強化式學習中扮演的指引目標的角色，由環境透過提供獎勵的方式來衡量代理人的表現並影響學習的方向。透過獎勵我們告訴代理人「要達成的是什麼」，而不是如何達成。如何達成目標的策略是代理人學習的成果。


## 2. 馬可夫決策過程 

以 \\(S\\) 代表 State（或 Observation）、\\(A\\) 代表 Action、\\(R\\) 代表 Reward，並考慮時間步驟 \\( t = 0,1,2,\...  \\) ，我們將代理人與環境的互動用符號作形式化的描述：根據觀察到的狀態 \\(S_0\\) 代理人採取了行動 \\(A_0\\)，接著環境給出了回饋 \\(R_1\\) 而新的狀態變成 \\(S_1\\)，依此類推。

$$ S_0 \xrightarrow{A_0} (R_1, S_1) \xrightarrow{A_1} (R_2, S_2) \xrightarrow{A_2} \... $$

或直接看成 \\(S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, \... \\) 這樣的序列軌跡。

這裡採取的時間標記方式將行動 \\(A_t\\) 之後得到的報酬表達為 \\(R_{t+1}\\) 而非 \\(R_t\\)，以強調獎勵作為行動後果的模型詮釋。

#### 動態函數

在 \\((S_t,R_t)\rightarrow(S_{t+1},R_{t+1}) \\) 一式中的「箭號」部分代表著環境本身的機制，稱為動態函數（dynamic functions）或是轉移函數（transition function），可用一個機率函數 \\(p\\) 將這個過程形式化為：

$$ p(s^\prime,r|s,a) = Pr \lbrace S_t=s^\prime, R_t=r | S_{t-1}=s, A_{t-1}=a \rbrace $$

意即在給定 \\(s,a\\) 的情況下，新狀態和獎勵組合 \\(s^\prime, r\\) 的機率分布是確定的。要注意的是，透過 \\(p\\) 所確定下來的是機率分布，並不是確定會進入某一個狀態和獎勵這樣的決定性轉換，換句話說 MDP 模型容許環境具有隨機性。

環境動態函數 \\(p\\) 是同時考慮了狀態和獎勵組合分佈的一般性敘述，亦可以定義隨機狀態轉移函數 \\(T(s,a,s^\prime)\\) 和獎勵函數 \\(r(s,a)\\) 來分別描述狀態和獎勵的變化。

#### 馬可夫性質
此一形式的成立依賴於 MDP 模型的核心假設「**馬可夫性質**」：**當前狀態包含所有與未來相關的資訊（The future is independent of the past, given the present）**，因此動態函數才會不需要考慮任何過去的歷史狀態，僅需要給定 \\(s,a\\) 為條件。


## 3. 目標與獎勵 

代理人與環境的互動是一個持續的過程，而環境獎勵也可能多次發生，因此代理人追求的目標應該是整個過程中累積的總獎勵。若從決策的角度來看，那麼在任何一個時刻 \\(t\\) ，過去發生的歷史也已經不重要，真正要關注的是你採取的行動如何讓未來的總獎勵最高，我們將這個目標稱為「回報」Return ，表達為
$$ G_t = R_{t+1} + R_{t+2} + R_{t+3} + \...$$
（記得 \\(R_{t+1}\\) 代表的就是時刻 \\(t\\) 當下採取行動 \\(A_t\\) 後得到的獎勵，我們並沒有漏掉 \\(R_t\\) ）

又基於以下兩個主要考量，模型引入介於零到一之間的折扣因子 \\(\gamma\\) 作為調節的參數
1. 互動不一定有明確的結束（不一定是回合性的）因此總獎勵可能會很大、無限大
2. 相對於不確定高的遙遠未來獎勵，我們應該更重視近期的獎勵

加入折扣因子後的回報可以寫作
$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \...$$

或整理為常見的形式
$$ G_{t} = \sum_{k=t+1}^{T}\gamma^{k-t-1} R_{k} $$
其中 \\(T\\) 代表互動終止的回合（\\(T\\) 可以是 \\(\infin\\)，只要同時 \\(\gamma\neq1\\) 就好）。

最大化「**累積折扣獎勵**」也就是強化式學習代理人的目標。


## 4. 馬可夫決策過程五元組

經歷了以上的討論，我們可以將 MDP 用 \\(\langle \cal{S}, \cal{A}, \mathit{T}, r, \gamma \rangle \\) 五元組表達。
- \\(\cal{S} \\) ＝ 狀態空間（state space，由狀態組成的集合，\\(S_t \in \cal{S}\\)）
- \\(\cal{A} \\) ＝ 行動空間（action space，由行動選擇組成的集合，\\(A_t \in \cal{A}\\)）
- \\(T\\) ＝ 隨機狀態轉移函數  
- \\(r\\) ＝ 回饋函數
- \\(\gamma\\) ＝ 折扣函數 

理解 MDP 的 \\(\langle \cal{S}, \cal{A}, \mathit{T}, r, \gamma \rangle\\)  內容有助於具體想像強化式學框架，類似於 \\(X \rightarrow y\\) 之於監督式學習。舉個例子來看會比較容易理解。

#### Atari 遊戲的 DQN Agent
- 代理人的觀察是遊戲畫面，\\(\cal{S} \\) 代表了值在 0～255 之間、大小為 210＊160＊3 的陣列們所構成的集合。
- 以 Gymnasium 套件來說其中定義了 18 種代理人夠選擇的遊戲操作動作，\\(\cal{A} \\) 代表這些行動構成的集合。
- 任何時候遊戲系統都能根據代理人的輸入計算出下一步的遊戲狀態以及分數的變化，故\\(T\\) 和 \\(r\\) 為遊戲系統本身。
- \\(\gamma\\) 是 MDP 模型的參數，可以簡單地選擇一個常數例如 0.99。

在這裡輕鬆帶過的 \\(T\\) 和 \\(r\\) 在現實應用上可能是最大的困難，因為這可能代表著需要花費大量心力建立必要的虛擬環境來提供給代理人互動學習。畢竟在現實世界裡失敗的成本往往太高，斷不可能為了訓練自動駕駛而真的讓汽車上路亂開。

## 5. 策略與價值函數

強化式學習中的代理人面對的課題是「如何選擇行動才能達到最大化回報 \\(G_t\\) 的目標」。\\(G_t\\) 代表了自時間 \\(t\\) 開始往後的互動結果中將會得到的折扣後獎勵總和，不同的行動選擇會導致不同的互動過程和獎勵並影響最終回報。

這一連串的選擇反映的是代理人的行動選擇「策略」，可以用一個函數 \\(\pi(a|s)\\) 來表達在給定狀態 \\(s\\) 之下代理人會如何選擇行動 \\(a\\)。有時很容易忘記 \\(\pi\\) 是機率分布函數，只要記得「隨機選擇」也是一種策略就好（例如上下左右各有 0.25 的機率）。

你也許會想，要是我知道在當前的狀態採取什麼行動能帶來的回報是多少，那就選擇回報最高的那個行動就好，但是我們並不知道每個行動的價值，所以這只是換句話說，並沒有解決問題。是這樣沒錯，但也不是這樣，將策略問題轉化爲價值估計的問題正是價值學習方法（value-based learning）的第一步。


#### 網格世界的思考

想像代理人在一個能夠上下左右移動的方形地圖上尋找寶藏，其所在的位置也同時代表了狀態，為了取得最大的累積折扣獎勵代理人應該如何思考？

對於一個處於狀態 \\(s_0\\) 的代理來說，假設已知選擇向上下左右移動時分別能得到獎勵 \\(r_{u}\\), \\(r_{d}\\), \\(r_{l}\\), \\(r_{r}\\)，那自然是獎勵較高者吸引人。但考慮到遊戲並非在一次互動後就結束而真正目標其實是「累積折扣獎勵」時，就需要同時考慮下一步時可以取得的回報，如此才不會為了眼前的好處而略了未來的獲利。

這也點出了很重要的遞回關係：當前的回報 \\(G_t\\) 可以用立即獎勵 \\(R_{t+1}\\) 和下一步起算的回報 \\(G_{t+1}\\) 表達：
$$ \begin{array}{lll} 
   G_t &=& R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \... \newline
       &=& R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \...) \newline
       &=& R_{t+1} + \gamma G_{t+1}
    \end{array}
$$
也就是說，我們需要比較的是採取不同行動的條件下各自算出來的 \\(R_{t+1}+\gamma G_{t+1}\\) 大小。

然而，目前的條件尚不足以將 \\(G_{t+1}\\) 確定下來，這是因為回報取決於後續每一個步驟所選取的行動而不只是單一次的選擇，因此我們真正得考慮的是不同行動策略下的回報。為此，我們定義狀態價值函數 \\(v_{\pi}(s)\\) 來代表狀態 \\(s\\) 在遵循策略 \\(\pi\\) 的情況下能得到的累積折扣獎勵。

假使我們令 \\(\pi_{u}\\) 代表總是選擇向上走的策略，\\(\pi_{d}\\) 代表總是選擇向下走的策略，並假設從 \\(s_0\\) 狀態開始兩種策略將分別導致進入新狀態 \\(s_u\\) 和 \\(s_d\\)，則藉由比較 \\(r_{u} + \gamma v_{\pi_{u}}(s_{u}) \\) 和 \\(r_{d} + \gamma  v_{\pi_{d}}(s_{d}) \\) 的大小，代理人將可以判斷哪個策略能帶來更高的回報。

#### 狀態價值函數

狀態價值函數 state-value function 的正式定義為
$$ v_{\pi}(s) = \mathbb{E}\_{\pi}[ G_t | S_t = s] $$

由於環境狀態的轉移函數、獎勵函數、以及策略本身都可能包含隨機性，故需要考慮取期望值 \\(\mathbb{E}\_{\pi}\\) 的概念。從隨機實驗的角度來想比較容易理解：\\( v_{\pi}(s) \\) 可以理解我們重複無數次實驗後得到的 \\(G_t\\) 平均值，其中每一次都是從狀態 \\(s\\) 開始並總是遵循策略 \\(\pi\\) 來選擇行動。

然而，要能夠直接使用狀態價值函數作為策略其實有先決條件，其中之一是代理人需要掌握環境的動態（dynamics，包含狀態轉移函數和獎勵函數），如此才能知道從 \\(s_0\\) 往上走後會得到獎勵 \\(r_u\\) 並且會進入狀態 \\(s_u\\)。

#### 行動價值函數

一個更直接的做法是估計「狀態加行動」的價值，我們定義行動價值函數 action-value function 為
$$ q_{\pi}(s,a) = \mathbb{E}_{\pi} [G_t | S_t = s, A_t = a ] $$

和狀態價值函數一樣，我們可以將 \\( q_{\pi}(s,a )\\) 想像為重複無數實驗後會得到的 \\(G_t\\) 平均值，其中每一次實驗都是從狀態 \\(s\\) 開始並且固定選擇行動 \\(a\\)，接下來遵循策略 \\(\pi\\) 來選擇行動。

使用行動價值函數作為策略很容易，只要拿它算出目前狀態下每個行動各自的價值，取最高即可。

## 6. 貝爾曼期望方程式 （Bellman Expectation Equation）

#### 展開下一步：狀態價值函數和行動價值函數的串聯

狀態價值 \\(v_\pi(s)\\) 描述的是狀態 \\(s\\) 的預期回報，而回報首先取決於你下一步的行動。假設在策略 \\(\pi\\) 之下你選擇的行動是 \\(a\\) ，則根據定義預期會得到回報就是 \\(q_\pi(s,a)\\)，注意行動價值表達的已經是長期的回報而不是單一步驟的獎勵，所以狀態價值 \\(v_\pi(s)\\) 也就等於行動價值 \\(q_\pi(s,a)\\)。

考慮到策略 \\(\pi\\) 實際上決定的是行動選擇的機率分布，狀態價值必須綜合考量所有行動（但不需考慮行動後進入的新狀態，因為這層考慮已由行動價值負責）：
$$ v_\pi(s) = \mathbb{E}_\pi [q(S_t,A_t)  | S_t = s ] = \sum_a \pi(a|s) q\_{\pi}(s,a)  $$

同樣的，行動價值函數 \\(q_\pi(s,a)\\) 也可以用狀態價值函數表達，假設我們採取行動 \\(a\\) 後得到立即獎勵 \\(r\\) 並進入了新狀態 \\(s^\prime\\)，同樣因為 \\(v_\pi(s^\prime)\\) 已經代表了進入狀態 \\(s^\prime\\) 後的長期回報，因此行動價值也就可以寫作 \\(r+v_\pi(s^\prime)\\)。精確的式子一樣需要考慮到環境動態的隨機性，因此使用期望值的方式表達：
$$ 
q_\pi(s,a) = \mathbb{E} [R_{t+1} + v_\pi(S_{t+1}) | S_t=s, A_t=a] 
    = \sum_{s^\prime,r} p(s^\prime,r|s,a)[r+v_\pi(s^\prime)]
$$

#### 展開下兩步：價值函數自身的遞迴

當我們使用環境動態（狀態轉換函數、獎勵函數）加上策略函數來表達可能的互動軌跡，並運用期望值加以收納時，我們看到 \\(v\\) 可以用 \\(q\\) 展開，而 \\(q\\) 也可以用 \\(v\\) 展開。在前面我們已經看過回報 \\(G_t\\) 的遞迴表達，價值函數 \\(v_\pi(s)\\) 和 \\(q_\pi(s,a)\\) 也可以和下一個時間步驟的價值函數串連起來，其結果為貝爾曼方程式。

$$ \begin{array}{lll}
v_{\pi}(s) & = & \mathbb{E}\_{\pi}[ G_t | S_t = s] \newline
    & = & \mathbb{E}\_{\pi}[ R_{t+1} + \gamma G_{t+1} | S_t = s] \newline
    & = & \sum_{a} \pi(a|s) \sum_{s^\prime,r} p(s^\prime, r | s, a)[r + \gamma \mathbb{E}\_{\pi} [G_{t+1} | S_{t+1} = s^\prime]] \newline
    & = & \sum_{a} \pi(a|s) \sum_{s^\prime,r} p(s^\prime, r | s, a)[r + \gamma v_\pi(s^\prime)]
\end{array}
$$

$$ \begin{array}{lll}
q_{\pi}(s,a) & = & \mathbb{E}\_{\pi}[ G_t | S_t = s, A_t = a] \newline
    & = & \mathbb{E}\_{\pi}[ R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] \newline
    % & = & \sum_{r,s^\prime} p(s^\prime, r | s, a) [ r ] \newline
    % &   & +\gamma \sum_{r,s^\prime} p(s^\prime, r | s, a) \sum_{a^\prime} \pi(a^\prime|s^\prime) 
    %         [ \mathbb{E}\_{\pi}[G_{t+1}|S_{t+1}=s^\prime,A_{t+1}=a^\prime]  ] \newline
    & = & \sum_{s^\prime,r} p(s^\prime, r | s, a) [ r + \gamma \sum_{a^\prime} \pi(a^\prime|s^\prime) 
          \mathbb{E}\_{\pi}[G_{t+1}|S_{t+1}=s^\prime,A_{t+1}=a^\prime]  ] \newline
    & = & \sum_{s^\prime,r} p(s^\prime, r | s, a) 
          [ r+ \gamma \sum_{a^\prime} \pi(a^\prime|s^\prime) q_{\pi}(s^\prime,a^\prime)  ]
\end{array}
$$

#### 策略評估：計算策略的狀態價值 

在環境動態已知的情況下，貝爾曼方程式可以用於計算遵循給定策略時所對應的狀態價值函數，稱為狀態評估 Policy Evaluation。

當我們使用實際問題中的 \\(p\\) 和 \\(\pi\\) 帶入貝爾曼方程式展開，就可以將每一個狀態的價值函數用其可能後續狀態的價值函數表達的關係式。假設問題包含 \\(N=|\cal{S}|\\) 個狀態，我們會得到 \\(N\\) 組 \\(N\\) 個未知數的聯立方程式，方程式的解就是策略的狀態價值。

透過迭代的方式來計算價值函數的演算法稱為 iterative policy evaluation，其做法是將貝爾曼方程式改視為一種數值更新的規則：
$$ v_{k+1} = \sum_{a} \pi(a|s) \sum_{s^\prime,r} p(s^\prime, r | s, a)[r + \gamma v_k(s^\prime)] $$
在 \\(k=0,1,2,\...\\) 的迭代步驟中，從隨機的值出發（但注意 terminal state 要一開始就設為零），不斷將當前的估計 \\(v_k\\) 帶入等號右邊以得到新一輪的估計 \\(v_{k+1}\\)，最終 \\(v_k\\) 將會逼近 \\(v_\pi\\)。


#### 強化式學習中的動態規劃

迭代策略評估 Iterative policy evaluation 方法屬於動態規劃 Dynamic Programming（DP）方法的一種。這是因為貝爾曼方程式的遞迴關係將「計算目標狀態價值的問題」拆解為「計算後續狀態狀態價值」子問題，因此具有 DP「最優子結構」（問題的最優解依賴於其子問題的最優解）和「重疊子問題」（子問題會重複出現，只需計算一次）的兩大特性。

我對動態規劃的理解來自傳統的資料結構與演算法，看過的問題都是最底層的子問題可以得到精確的解，並直接被用於建構更大問題的解這樣的模式。然而在強化式學習的情境中並不存在似類似的可以被精確解出的最底層問題，因此很困惑為何兩種都被稱為動態規劃問題。這兩著之間的關鍵差異可以用「是否存在狀態的循環依賴」來理解。

不同於傳統演算法的動態規劃問題，MDP 問題裡雖然將當下狀態的價值問題拆解為下一步狀態的價值問題，但狀態之間的依賴（通常）存在循環（而非有向無環圖），因此並沒有一個終點可以直接取值。迭代策略評估之中的「迭代」目的就是從一個初始估計一步步逼近收斂到真正的解，而這種方法的成立則奠基於「基於貝爾曼方程式的更新的規則」是一個收縮映射的過程。

## 7. 貝爾曼最優方程式（Bellman Optimality Equation）

獎勵是目標的訊號，回報是代理人的目標，價值函數提供了個別狀態或是在個別狀態下採取特定行動時預期得到的回報。作為一種深思熟慮的對未來的計算，價值函數將長期的不確定性資訊用期望值收斂到眼前單一的數字，為代理人的行動提供了完美的指引。

當一個策略 \\(\pi\\) 的價值函數 \\(v_\pi(s)\\) 在所有的狀態下都勝過或等於另外一個策略 \\(\pi^\prime\\) 的價值函數時，我們說 \\(\pi \ge \pi^\prime\\)：策略 \\(\pi \\) 優於策略 \\(\pi^\prime\\)。又在無數的可能策略之中，至少會存在一個最優策略 \\(\pi_\ast\\)，他所對應到的價值函數 \\(v_\ast(s)\\) 在所有的狀態上都是最佳，同時他的行動價值函數 \\(q_\ast(s,a)\\) 在所有的狀態行動組合上也都是最佳的。
$$ v_\ast(s) = \max_{\pi} v_{\pi}(s) \newline q_\ast(s,a) = \max_{\pi}q_\pi(s,a) $$

當採取最佳策略時，我們可以將貝爾曼方程式中關於行動期望值的部分（\\(\sum_a \pi(s,a)\\)）改為選擇最佳行動，得到的結果稱為貝爾曼最優方程式。

$$ v_\ast(s) = \max_a \sum_{s^\prime,r} p(s^\prime, r|s,a) [r + \gamma v_\ast(s^\prime)] $$

$$ q_\ast(s,a) = \sum_{s^\prime,r} p(s^\prime, r|s,a) [r + \gamma \max_{a^\prime} q_\ast(s^\prime, a^\prime) ] $$

#### 價值迭代：解最優策略

透過使用貝爾曼最優方程進行迭代策略評估，我們可以計算最優策略 \\(\pi_\ast\\) 下的狀態價值函數 \\(v_\ast(s)\\)，這稱為「價值迭代」 value iteration

$$ v_{k+1}(s) = \max_a \sum_{s^\prime,r} p(s^\prime, r|s,a) [r + \gamma v_k(s^\prime)] $$

待方法收練得到最優價值函數後，就可以進一步從中提取出最佳策略：

$$ \pi_\ast(a|s) = \argmax_{a} \sum_{s^\prime,r} p(s^\prime, r|s,a) [r + \gamma v_\ast (s^\prime) ] $$

#### 策略迭代：解最優策略

另一種計算最優策略的方法是交替進行「策略評估」和「策略改善」兩種步驟直到收斂，稱為「策略迭代」 policy iteration 。

$$ \pi_0 \xrightarrow{\text E} v_{\pi_{0}} \xrightarrow{\text I} 
   \pi_1 \xrightarrow{\text E} v_{\pi_{1}} \xrightarrow{\text I} 
   \pi_1 \xrightarrow{\text E} v_{\pi_{2}} \xrightarrow{\text I} \...
$$ 

1. **策略評估 policy evaluation（\\(\xrightarrow{\text E}\\)）：固定策略 \\(\pi_k\\)，計算出符合當下策略的狀態價值 \\(v_{\pi_{k}}\\)**

    可以套用前面介紹過的迭代策略評估方法，在固定 \\(\pi_k\\) 的情況下，迭代 \\(l=0,1,2,\...\\) 使 \\(v_l\\) 最終收斂到 \\(v_{\pi_k} \\)：
    $$ v_{l+1} = \sum_{a} \pi_{k}(a|s) \sum_{s^\prime,r} p(s^\prime, r | s, a)[r + \gamma v_l(s^\prime)] $$
    （這裡改用 \\(l\\) 代表此階段內的迭代過程，避免和 \\(k\\) 混淆。）

2. **策略改善 policy improvement（\\(\xrightarrow{\text I}\\)）：固定當下狀態價值函數 \\(v_{\pi_{k}}\\)，計算出對應的最佳策略做為新策略**

    策略改善的做法則是「總是選取當下能帶來最高行動價值的那個行動」，也就是貪婪的概念（greedy）：
    $$ \pi_{k+1}(s) = \argmax_a \sum_{s^\prime,r} p(s^\prime,r|s,a) [r+\gamma v_{\pi_{k}}(s^\prime)  ] $$

當策略迭代方法收斂時 \\(v\\) 和 \\(\pi\\) 不再隨著計算而改變，此時策略和策略價值達到一致（否則策略評估步驟會改變 \\(v\\)），並且策略等同於貪婪的選擇最大價值的行動（否則策略改善會導致 \\(\pi\\) 的改變），因此最終的 \\(v\\) 和 \\(\pi\\) 就滿足了貝爾曼最優方程式
$$ v_\ast(s) = \max_a \sum_{s^\prime,r} p(s^\prime, r|s,a) [r + \gamma v_\ast(s^\prime)] $$


#### 廣義的策略迭代

如果將「策略評估」和「策略改善」廣義的視為兩種階段（而非策略迭代法中精確的步驟），則構成了「廣義策略迭代」generalized policy iteration （GPI）的概念

$$ \text{Policy Evaluation} \rightleftarrows \text{Policy Improvement} $$

GPI 可以幫助我們思考不同強化式學習方法的框架，例如價值迭代法就可以理解為一種廣義策略迭代，和前述策略迭代的差異在於其策略評估階段只迭代更新一次而不是跑到收斂為止。
$$ v_{k+1}(s) = \max_a \sum_{s^\prime,r} p(s^\prime, r|s,a) [r + \gamma v_k(s^\prime)] $$
上式中的 \\(\max_a\\) 對應到策略改進步驟：將策略設定為貪婪的選取當下最佳價值；而策略評估步驟就是 \\(v_{k} \rightarrow v_{k+1}\\) 的更新，因此價值迭代方法裡每更新一次價值函數（迭代一次而不是跑到收斂），就會更新一次策略。

#### 最優解的裡想

目前為止的方法讓我們可以利用價值函數和貝爾曼方程式計算最優策略，可惜這些方法在實際問題上的可行性通常有限。

第一個限制就在於我們必須知道環境的動態函數才能進行價值的計算，這類型的方法也被稱為 model-based 方法，因為需要有一個對世界運作機制精確的知識與描述的 world model。

第二個限制在於所需要的計算資源。價值迭代方法需要對「所有的狀態」進行多輪的計算及更新，因此在狀態數量大的問題上所需的時間和記憶體資源也將變得很巨大。

雖然最優解在我們真正感興趣的強化式學好問題上通常是無法達到的理想，但只要利用實際互動經驗替代無法取得的環境動態函數，就能夠以近似的方式來近似的解貝爾曼最優方程。

## 8. 蒙地卡羅方法

在沒有環境動態函數的情況下我們無法直接進行狀態函數期望值相關的計算，但我們可以透過重複實驗並統計結果的方式模擬環境動態函數的效果，這就是蒙地卡羅方法的精神：以隨機實驗結果的樣本發生頻率來估計事件發生的機率。

如果你想知道擲硬幣時正反面發生機率是否真的相同，透過不斷實驗、紀錄並統計最終正面和反面各自的發生次數，我們就可以逼近兩者發生的機率。每一次的隨機實驗都像是對機率函數的一次抽樣，隨著實驗次數的增加，平均結果也會越來越接近理論上的期望值。

#### 狀態價值函數的估計

在有明確終止條件的環境類型中，我們可以利用蒙地卡羅法來估計給定策略的價值函數 \\(v_\pi(s)\\)。每一次遵循策略 \\(\pi\\) 並完成一輪完整的互動（episode），就如同進行了一次隨機實驗的採樣。我們紀錄互動歷程中遭遇之狀態及得到的獎勵的軌跡 \\(s,a,r,\...\\)，就可以在終局時回頭還原每一個狀態 \\(s_t\\) 下最終得到的累積折扣獎勵，並當成一組「狀態＋回報」的配對資料加入我們的統計中。最後再將同一個狀態下所有的回報數據取平均，就得到了狀態價值的估計。


#### 行動價值函數的估計

同樣的方法也可以用於估計行動價值函數 \\(q_\pi(s,a)\\)。我們可以直接使用隨機實驗的統計數據來評估每次遇到狀態 \\(s\\) 且採取行動 \\(a\\) 然後接下來都遵守策略 \\(\pi\\) 時會得到的平均回報會是多少。行動價值的一個好處是可以直接的轉化爲行動策略，只要貪婪的選擇最高價值的行動即可，不像價值函數還需要配合獎勵函數才能估計下一步的回報來作為選擇行動的依據。

不過如果每次都遵守特定的行動選擇策略，可能會導致某些狀態行動 \\((s, a)\\) 配對從未被選擇過，我們便可能因此錯過了更佳的行動選擇。要避免這種狀況我們需要持續的探索，理想上我們想要讓每次實驗的初始狀態 \\((s, a)\\) 涵蓋所有可能的情境組合，實際上的做法則例如 \\(\epsilon\\)-greedy。

#### 蒙地卡羅方法的特性

這種透過蒙地卡羅方法來估計每個狀態的回報的方式和動態規劃有一項性質上的差別，那就是對於每個狀態的價值估計都是獨立的，透過紀錄每一次遭遇後得到的，在動態規劃中價值函數的估計取決於其他狀態的價值函數。

採用蒙地卡羅方法來採樣和使用完整的轉移機率來計算期望值有一點重要的不同：隨機實驗根據樣本來做價值函數的估計，而樣本的分布會自然傾向發生機率高的事件。這雖然代表我們不容易正確的評估出現機率低的狀態的價值，但也代表在狀態空間非常大的狀態下，我們仍可以很快的掌握主要的問題情境而不被大量鮮少出現的狀態所拖累。

<!-- #### on-policy vs off-policy -->

## 9. 時序差分學習

強化式學習的目標是找尋能最大化累積折扣獎勵的策略，目前為止我們已經建立了一套參考框架。
- 透過價值函數的定義，問題轉化為如何找到最優的價值函數（包含狀態價值函數或者是動作價值函數），而最優策略就是貪婪的選擇最高價值的下一步。
- 廣義的策略迭代可以用來計算最優策略，其做法是反覆兩種步驟的交替迭代：第一是策略評估，目標在於讓價值函數和當下的策略一致；第二則是策略改善，目標是更新策略為以當下價值函數貪婪的做選擇。

不同強化式學習方法間主要的差異通常在於進行策略評估的方法，也就是如何估計價值函數。以下價值函數的表達形式由上而下分別大概展現了蒙地卡羅、本節的主角時序差分（Temporal-Difference TD）、以及動態規劃三種方法各自的學習目標
$$
\begin{array}{lll}
v\_\pi(s) & = & \mathbb{E}\_\pi [G_{t} | S_t=s]  \newline
  &=& \mathbb{E}\_\pi [R_{t+1} + \gamma G_{t+1} | S_t=s] \newline
  &=& \mathbb{E}\_\pi [R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s]
\end{array}
$$

動態規劃方法（第三行）透過計算當下狀態價值的期望值來更新。由於使用到其他狀態價值的估計，故具有自舉（自助法）的性質 bootstrapping。又因為需要計算回報的期望值來更新（稱為 expected updates），因此會需要環境動態函數的知識。

蒙地卡羅方法的目標（第一行）則是直接更新狀態價值。因為是使互動實驗中得到的實際回報樣本做更新（稱為 sample update）而不是真的計算期望值，因此不依賴關於環境動態函數的知識。又更新公式中沒有使用到其他狀態的價值估計，因此個別狀態價值估計的更新是獨立、非自助式的 non-bootstrapping。

假使考慮到環境可能是非穩定的（環境動態函數可能隨時間改變），這個學習的過程更適合用增量式學習（incremental learning）的方式表達
$$ V(S_t) \leftarrow V(S_t) + \alpha [G_{t} - V(S_t)] $$
其中 \\(G_t\\) 是我們讓 \\(V\\) 學習的的目標，\\(\alpha\\) 是個參數，代表學習率或更新步伐。

時序差分學習（第二行）是一種結合了兩者特性的方法，也構成了現代強化學習務實的基本技術。他和蒙地卡羅一樣採用 sample update 從實際互動樣本經驗中學習，因此沒有 DP 需要依賴於環境動態函數的問題，而他的更新方式和動態規劃一樣會基於於其他狀態估計值（bootstrapping），故不需要像 MC 方法一樣等待回合最終的回報才能做學習。公式表達如下
$$ V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)] $$

可以看到 MC 和 TD 方法一樣都是用實驗樣本做更新，但作學習為目標的分別為 \\(G_{t}\\) 和 \\(R_{t+1} + \gamma V(S_{t+1})\\)。我們可以將原本的估計值和目標之間的差異想成一種誤差，定義 TD Error 為 
\\( \delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \\)。

#### TD 方法學習行動價值

如果我們關注的是控制問題（control problem），學習行動價值函數是更直接的選項。行動價值的定義為 \\( q_\pi(s,a) = \mathbb{E}\_\pi [ R_{t+1}+\gamma G_{t+1}|S_t=s, A_t=a] \\)，其策略評估的目標的形式和價值函數是相同的。但是為了讓實驗能夠探索涵蓋所有的 \\((s,a)\\) 組合，要採取例如 \\(\epsilon\\)-greedy 方式的策略。

#### SARSA 方法
在同策略 on-policy 學習的情況下，因為行動的選擇（\\(A_t\\) 和\\(A_{t+1})）都是根據當下的策略，在策略評估時我們可以直接採用以下的更新規則

$$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t, A_t)] $$

但也因為 SARSA 學習的是當下策略（例如 \\(\epsilon\\)-greedy）策略的價值，必須讓 \\(\epsilon\\) 在長期時下降到零才會趨近於我們真正的目標最優策略。

#### Q-learning 方法

在異策略 off-policy 學習的情況下，因為訓練樣本數據的來源並非是當下的策略（例如可能是探索策略），故我們需要以 \\(\max_a\\) 代表（最佳的）完全貪婪策略來選擇下一步以正確的計算目標價值。更新規則如下，稱為 Q-learning 方法
$$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1},a) - Q(S_t, A_t)] $$

Q-learning 方法直接學習最佳策略 \\(\pi_\ast\\)




{{< katex >}}