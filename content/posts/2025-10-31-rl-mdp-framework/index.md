+++
date = '2025-10-23T09:39:41+08:00'
title = '強化式學習：MDP 框架'
tags = ['ml', 'read', 'zh']
draft = true
+++

## Why Reinforcement Learning？

大約兩個多月前，我在[強化式學習雜記]({{< ref "posts/2025-09-10-reinforcement-learning/index.md">}})一文中紀錄了我的一些想法，但沒有提到我選擇這個題目的動機：**除了學習新知本身的樂趣以外，我想要擴展我原本對機器學習的認識以包含強化式學習的框架，多一種的看待問題的角度**。

#### 監督式學習的框架

廣義來說，模型是人類對各種複雜現象的本質嘗試理解與簡化，提供了我們得以思考、評估、甚至量化並做出預測幫助解決問題的框架。

包含深度學習的回歸和分類器在內的監督式機器學習成果告訴我們，只要問題拆解後能夠以
$$ X \rightarrow y $$ 
的輸入輸出配對方式表達，並且取得足量的有效資料，那我們就有機會訓練出具一定泛化能力的擬合模型。即使不同的情境及資料型態會有各自的限制以及眉角，但核心概念都是從資料中學習由輸入預測輸出，找到一個良好的映射函數。

#### 強化式學習的世界

強化式學習試圖處理的問題是「代理人如何透過與環境的互動和回饋的引導學習出好的行動策略」。

對人類的日常經驗來說，依照回饋來修正自己行爲並不陌生，強化式學習乍看之下也和監督式學習頗爲相似，然而在一連串的互動和偶爾得到獎勵的情境下，學習該如何進行？要拿什麼作為輸入？又該如何定義答案？稍微思考後便會發現這過程並不簡單，也無法用函數擬合的方式直接想像。

原因無他，強化式學習問題本身就是一個更複雜的情境，因此需要額外的模型框架來描述，而這個關鍵的模型就是**馬可夫決策過程 Markov Decision Process（MDP）**。

MDP 為問題框架建立了形式化的數學框架，以此為基礎我們得以進一步
- 定義**期望折扣報酬 Expected Discounted Return** 以作為學習目標
- 理解**狀態 state value 價值、行動價值 state-action value** 的意義和值學習方法
- 以**貝爾曼期望方程式 Bellman equation** 為基礎，進行**時序差分學習 Temporal Difference Learning**

在問題的最後所需要的深度學習計算依舊能夠以函數擬合來思考，但要理解強化式學習的整體框架，還需要更多的觀念作為階梯。

本文紀錄我串聯這些觀念的嘗試，主要參考文獻爲 Sutton & Barto Reinforcement Learning - An Introduction 第三章。

---

## 1. 環境與代理人

{{< mermaid >}}
graph LR
    A[Agent] -- Action --> E[Environment] 
    E -- Reward --> A
    E -- Observation --> A
{{< /mermaid >}}

在強化式學習的問題中，首先要區別「環境（Environment）」與「代理人（Agent）」並了解兩者如何互動。代理人是學習的主角，環境則是代理人所在的世界或是系統，決定了代理人能取得的外在資訊。以遊戲的情境來說代理人就是玩家或者是遊戲 AI，環境則是遊戲本身。

#### 兩者之間的互動介面包含三種資訊
1. 觀察（Observation）又稱狀態（State）
2. 行動（Action）
3. 獎勵（Reward）又稱回饋

這三項資訊是唯一需要通過邊界的訊號。代理人對環境的了解僅限於觀察和獎勵，而環境會根據其機制以根據代理人的行動決定新的狀態並提供對應的回饋，其自身對代理人來說很像某種黑盒子。就好比駕駛人會知道自己踩下油門並看到汽車加速前進以及是否出車禍，但並不會、也不需要知道汽車是如何運作的。

#### 獎勵作為目標的信號

當你漫無目的上路，觀察到的狀態景色會隨著你不同的行動而改變，但你選擇的路徑並沒有所謂好壞可言，因為沒有任何衡量基準在。當你的目標是「盡快回家」時，就產生了最短路線和繞遠路的差別；又若你的目標是「平安到家」，則安全駕駛就會優於搶快超車。

「獎勵」在強化式學習中扮演的指引目標的角色，由環境透過提供獎勵的方式來衡量代理人的表現並影響學習的方向。透過獎勵我們告訴代理人「要達成的是什麼」，而不是如何達成。如何達成目標的策略是代理人學習的成果。


## 2. 馬可夫決策過程 

以 \\(S\\) 代表 State（或 Observation）、\\(A\\) 代表 Action、\\(R\\) 代表 Reward，並考慮時間步驟 \\( t = 0,1,2,\...  \\) ，我們將代理人與環境的互動用符號作形式化的描述：根據觀察到的狀態 \\(S_0\\) 代理人採取了行動 \\(A_0\\)，接著環境給出了回饋 \\(R_1\\) 而新的狀態變成 \\(S_1\\)，依此類推。

$$ S_0 \xrightarrow{A_0} (R_1, S_1) \xrightarrow{A_1} (R_2, S_2) \xrightarrow{A_2} \... $$

或直接看成 \\(S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, \... \\) 這樣的序列軌跡。

這裡採取的時間標記方式將行動 \\(A_t\\) 之後得到的報酬表達為 \\(R_{t+1}\\) 而非 \\(R_t\\)，以強調獎勵作為行動後果的模型詮釋。

#### 動態函數

在 \\((S_t,R_t)\rightarrow(S_{t+1},R_{t+1}) \\) 一式中的「箭號」部分代表著環境本身的機制，稱為動態函數（dynamic functions）或是轉移函數（transition function），可用一個機率函數 \\(p\\) 將這個過程形式化為：

$$ p(s^\prime,r|s,a) = Pr \lbrace S_t=s^\prime, R_t=r | S_{t-1}=s, A_{t-1}=a \rbrace $$

意即在給定 \\(s,a\\) 的情況下，新狀態和獎勵組合 \\(s^\prime, r\\) 的機率分布是確定的。要注意的是，透過 \\(p\\) 所確定下來的是機率分布，並不是確定會進入某一個狀態和獎勵這樣的決定性轉換，換句話說 MDP 模型容許環境具有隨機性。

環境動態函數 \\(p\\) 是同時考慮了狀態和獎勵組合分佈的一般性敘述，亦可以定義隨機狀態轉移函數 \\(T(s,a,s^\prime)\\) 和獎勵函數 \\(r(s,a)\\) 來分別描述狀態和獎勵的變化。

#### 馬可夫性質
此一形式的成立依賴於 MDP 模型的核心假設「**馬可夫性質**」：**當前狀態包含所有與未來相關的資訊（The future is independent of the past, given the present）**，因此動態函數才會不需要考慮任何過去的歷史狀態，僅需要給定 \\(s,a\\) 為條件。


## 3. 目標與獎勵 

代理人與環境的互動是一個持續的過程，而環境獎勵也可能多次發生，因此代理人追求的目標應該是整個過程中累積的總獎勵。若從決策的角度來看，那麼在任何一個時刻 \\(t\\) ，過去發生的歷史也已經不重要，真正要關注的是你採取的行動如何讓未來的總獎勵最高，我們將這個目標稱為「回報」Return ，表達為
$$ G_t = R_{t+1} + R_{t+2} + R_{t+3} + \...$$
（記得 \\(R_{t+1}\\) 代表的就是時刻 \\(t\\) 當下採取行動 \\(A_t\\) 後得到的獎勵，我們並沒有漏掉 \\(R_t\\) ）

又基於以下兩個主要考量，模型引入介於零到一之間的折扣因子 \\(\gamma\\) 作為調節的參數
1. 互動不一定有明確的結束（不一定是回合性的）因此總獎勵可能會很大、無限大
2. 相對於不確定高的遙遠未來獎勵，我們應該更重視近期的獎勵

加入折扣因子後的回報爲
$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \...$$
或整理為常見的形式
$$ G_{t} = \sum_{k=t+1}^{T}\gamma^{k-t-1} R_{k} $$
其中 \\(T\\) 代表互動終止的回合（\\(T\\) 可以是 \\(\infin\\)，只要同時 \\(\gamma\neq1\\) 就好）。

最大化「**累積折扣獎勵**」也就是強化式學習代理人的目標。


## 4. 馬可夫決策過程五元組

經歷了以上的討論，我們可以將 MDP 用 \\(\langle \cal{S}, \cal{A}, \mathit{T}, r, \gamma \rangle \\) 五元組表達。
- \\(\cal{S} \\) ＝ 狀態空間（state space，由狀態組成的集合，\\(S_t \in \cal{S}\\)）
- \\(\cal{A} \\) ＝ 行動空間（action space，由行動選擇組成的集合，\\(A_t \in \cal{A}\\)）
- \\(T\\) ＝ 隨機狀態轉移函數  
- \\(r\\) ＝ 回饋函數
- \\(\gamma\\) ＝ 折扣函數 

理解 MDP 的 \\(\langle \cal{S}, \cal{A}, \mathit{T}, r, \gamma \rangle\\)  內容有助於具體想像強化式學框架，類似於 \\(X \rightarrow y\\) 之於監督式學習。舉個例子來看會比較容易理解。

#### Atari 遊戲的 DQN Agent
- 代理人的觀察是遊戲畫面，\\(\cal{S} \\) 代表了值在 0～255 之間、大小為 210＊160＊3 的陣列們所構成的集合。
- 以 Gymnasium 套件來說其中定義了 18 種代理人夠選擇的遊戲操作動作，\\(\cal{A} \\) 代表這些行動構成的集合。
- 任何時候遊戲系統都能根據代理人的輸入計算出下一步的遊戲狀態以及分數的變化，故\\(T\\) 和 \\(r\\) 為遊戲系統本身。
- \\(\gamma\\) 是 MDP 模型的參數，可以簡單地選擇一個常數例如 0.99。

在這裡輕鬆帶過的 \\(T\\) 和 \\(r\\) 在現實應用上可能是最大的困難，因為這可能代表著需要花費大量心力建立必要的虛擬環境來提供給代理人互動學習。畢竟在現實世界裡失敗的成本往往太高，斷不可能為了訓練自動駕駛而真的讓汽車上路亂開。

## 5. 策略與價值函數

強化式學習考慮的是代理人如何選擇行動以達到最大化回報－也就是累積折扣獎勵－的目標。

你也許會想，要是我知道採取的行動能帶來多少回報，那我就選擇回報最高的那個行動就好，但問題是我們並不知道每個行動的價值，所以這並沒有解決問題。的確是如此，但將策略的問題轉化爲價值估計的問題正是「價值學習」 value-based learning 的概念。

累積折扣獎勵 \\(G_t\\) 代表了自時間 \\(t\\) 起的互動歷程中將會得到的折扣後獎勵總和，不同的策略在同樣的環境中會產生不同的互動序列結果，因此得到不同的回報：好的策略代表能得到更高的回報，不好的策略則較低。

我們用 \\(\pi(a|s)\\) 表達策略：給定狀態 \\(s\\) 時將會選擇採取什麼行動 \\(a\\)。對於策略 \\(\pi\\) 可以定義價值函數 state-value function：
$$ v_{\pi}(s) =\mathbb{E}_{\pi}[ G_t | S_t = s] $$

以及行動價值函數 action-value function：
$$ q_{\pi}(s,a) = \mathbb{E}_{\pi} [G_t | S_t = s, A_t = a ] $$

環境狀態的轉移、取得獎勵、策略本身都可以有隨機性，故以 \\(\mathbb{E}_{\pi}\\) 取隨機變數在遵循策略 \\(\pi\\) 的情況下的期望值。

價值函數 \\(v_{\pi}(s)\\) 和 \\(q_{\pi}(s,a)\\) 的重點在於其和終極優化目標 \\(G_t\\) 的一致，當我們能夠正確的估計狀態或是行動的價值函數，就等於有了得高分的策略－總是選最高價值的那就對了。

## 6. 貝爾曼方程和時序差分學習

在了解了 MDP 框架、學習的目標、價值函數的概念之後，終於可以開始回答強化學「習如何學習」，最後一塊拼圖是於貝爾曼方程的時序差分學習方法。





{{< katex >}}