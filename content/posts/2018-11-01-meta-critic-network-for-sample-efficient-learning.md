+++
date = '2018-11-01T00:00:00+08:00'
title = 'Meta-Critic Networks for Sample Efficient Learning'
tags = ['read', 'ml', 'zh']
+++

> å»¶ä¼¸å¼·åŒ–å­¸ç¿’ä¸­çš„ actor-critic æ¶æ§‹ï¼Œç”¨åŒä¸€å€‹ meta-critic æŒ‡å°è¨±å¤šåŒé¡å‹çš„ä¸åŒå·¥ä½œï¼Œç›®æ¨™æ˜¯è¨“ç·´ä¸€å€‹å¯ä»¥æœ‰æ•ˆæŒ‡å°æ–° few-shot è¨“ç·´å•é¡Œçš„ meta-criticã€‚ç‚ºäº†è®“ meta-critic å¯ä»¥å€åˆ¥ä¸åŒ task çš„å·®ç•°ï¼Œå¿…é ˆæŠŠã€Œå­¸ç¿’è»Œè·¡ï¼ˆlearning tracesï¼‰ã€encode å¾Œä¹Ÿä½œç‚º meta-critic çš„è¼¸å…¥ã€‚
> 
> å› æ­¤ Meta-critic åŒ…å«å…©éƒ¨åˆ†ï¼šï¼ˆ1ï¼‰å°æ‡‰å‚³çµ±çš„ value function çš„ Meta-Value Networkï¼Œä»¥åŠï¼ˆ2ï¼‰è² è²¬ encode å­¸ç¿’è»Œè·¡ï¼ˆstate, action, reward çš„æ­·å²ï¼‰å†è¼¸å…¥çµ¦ MVN çš„ Task-Action Encoder Networkã€‚å¯¦é©—æ–¼ sine & linear function regressionï¼ˆç›£ç£å¼å­¸ç¿’ï¼‰ä»¥åŠ cartpole control éŠæˆ²å•é¡Œï¼ˆå¼·åŒ–å­¸ç¿’ï¼‰ã€‚


| æ–‡ç«  |     |
|-----|-----|
| æ¨™é¡Œ | Learning to Learn: Meta-Critic Networks for Sample Efficient Learning |  
| ä½œè€… | Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, Yongxin Yang (2017) |
| é€£çµ | [arXiv:1706.09529](https://arxiv.org/abs/1706.09529) |

## ä¸»è¦æ€è·¯

#### ä¸€ã€å•é¡Œ

* AI æ²’è¾¦æ³•å¾ˆã€Œæœ‰æ•ˆç‡ã€çš„å­¸ç¿’æ–°çš„å·¥ä½œ
    * å°ç›£ç£å¼å­¸ç¿’ï¼ˆSLï¼‰ä¾†èªªï¼Œé€™è¡¨ç¤ºè¨“ç·´ä¸€å€‹æ–°æ¨¡å‹é€šå¸¸éœ€è¦å¾ˆå¤šè³‡æ–™ï¼ˆdataï¼‰ã€‚
    * å°å¼·åŒ–å­¸ç¿’ï¼ˆRLï¼‰ä¾†èªªï¼Œé€™è¡¨ç¤ºéœ€è¦èˆ‡ç’°å¢ƒäº’å‹•éå¸¸å¤šæ¬¡ï¼ˆinteractionï¼‰ã€‚

#### äºŒã€ç›®æ¨™

* è®“ä¸€å€‹ meta-learner (ceta-critic) åœ¨ç‰¹å®š Classï¼ˆä½†ä¸ç›¸åŒçš„ï¼‰è¨±å¤šå·¥ä½œä¸Šè¨“ç·´ learner (actor)ï¼Œè—‰æ­¤å­¸æœƒé—œæ–¼æ­¤å•é¡Œç¾¤çš„ç‰¹æ€§ã€‚
* è‘—é‡åœ¨ few-shot learning çš„æ”¹å–„ï¼šå¾€å¾Œåœ¨é¢å°ï¼ˆåŒ Class çš„ï¼‰æ–°å•é¡Œæ™‚ï¼Œå³ä½¿æ¨£æœ¬å¾ˆå°‘ï¼ˆSLï¼‰æˆ–å®¹è¨±äº’å‹•æ¬¡æ•¸è¼ƒå°‘ï¼ˆRLï¼‰ï¼Œä¹Ÿèƒ½æœ‰æ•ˆçš„è¨“ç·´å‡ºå¥½çš„ç¶²è·¯ã€‚

#### ä¸‰ã€æ–¹æ³•

* å‚³çµ± criticï¼ˆä¸€å€‹åƒ¹å€¼ç¶²è·¯ï¼‰åªè€ƒæ…® state å’Œ actionï¼Œç¾ä½¿ç”¨ä¸€å€‹è·¨å·¥ä½œçš„ meta-critic ä¸¦æŠŠç•¶ä¸‹ learning tracde ä¹Ÿç•¶ä½œè¼¸å…¥è³‡è¨Šä»¥ä¾›å€åˆ¥ã€‚
* meta-critic ç”±å…©å€‹ network çµ„æˆï¼š
    1. Meta-Value Networkï¼ˆMVN, or value netï¼‰ï¼šä»¥MLPå¯¦ä½œçš„åƒ¹å€¼ç¶²è·¯ã€‚è®“ actor æ“šæ­¤èª¿æ•´å…¶è¡Œå‹•ç­–ç•¥ policyã€‚
    2. Task-Actor Encoder Networkï¼ˆTAEN, or task netï¼‰ï¼šä»¥ LSTM å¯¦ä½œçš„ encoderã€‚è¼¸å…¥ä¸€ç³»åˆ—çš„å­¸ç¿’æ­·å² (state, action, reward) tripletsï¼Œè¼¸å‡º embedded feature çµ¦ MVN ä½¿ç”¨ ã€‚ç”±æ–¼å„ç¨®å•é¡Œéƒ½æœƒæœ‰ state, action, rewardï¼Œä¸æœƒé™ç¥ç¶“ç¶²è·¯ä½¿ç”¨çš„æ§‹é‹ã€‚
   
#### å››ã€çµæœ

1. SLï¼šåœ¨äººå·¥åˆæˆçš„ \\(a\sin(x+b)\\) èˆ‡ \\(cx+d \\) å‡½æ•¸é›†è³‡æ–™ä¸Šä½œ Regression å•é¡Œï¼ˆground truthå‡½æ•¸çš†ç‚ºsine å’Œ linearï¼Œä½†ä¸åŒ task çš„ \\(a,b,c,d\\) ä¸åŒï¼‰ã€‚meta-learning åœ¨æ­¤å°æ‡‰åˆ°çš„æ˜¯å­¸æœƒ sine å‡½æ•¸åŠç›´ç·šçš„ä¸€ç­ç‰¹æ€§ã€‚ 
    * è¨“ç·´å¥½çš„ meta-critic åœ¨æ¸¬è©¦å‡½æ•¸ä¸Šèƒ½å¤ å¾å¾ˆå°‘ç­†è³‡æ–™é»ï¼ˆä¾‹å¦‚ 4-shotï¼‰ä¼°è¨ˆå‡ºæ¥è¿‘ gound truth çš„ \\(a,b,c,d\\)ï¼Œè¡¨ç¤º meta-training æ™‚ç¢ºå¯¦æœ‰å­¸åˆ°æ±è¥¿ã€‚ 
2. RLï¼šåœ¨ Cartpole Control å•é¡Œä¸Šï¼Œä½¿ç”¨ meta-critic å­¸ç¿’å¾—æ›´å¿«ï¼ˆreward çš„é€²æ­¥ per training episode æ›´é«˜ï¼‰ï¼Œæœ€çµ‚æ¸¬è©¦ä¸­çš„ success rate ä¹Ÿæ¯”å…¶ä»–æ–¹æ³•é«˜å‡ºè¨±å¤šã€‚
    * æœ‰è¶£çš„æ˜¯ TAEN çš„ embedding \\(z\\) é¡¯ç¤º meta-critic å­¸æœƒçš„å€åˆ¥å¯ä»¥å°æ‡‰åˆ°ä¸åŒé•·åº¦çš„ pole lengthï¼Œé€™ä¸¦ä¸æ˜¯ actor å’Œ critic çŸ¥é“çš„è³‡è¨Šï¼
 
#### äº”ã€è²¢ç»
* æå‡ºä¸€ç¨®æœ‰å½ˆæ€§çš„ã€æ–°çš„ knowledge transfer é€”å¾‘ã€‚å¯é‹ç”¨åœ¨ SL å’Œ RL çš„ few-shot learning å•é¡Œä¸Šã€‚


## å…¶ä»–æ¦‚å¿µ

### å¾ Critic åˆ° Meta-Critic 

*å‚³çµ± RL Actor-Critic æ¶æ§‹*

* actorï¼ˆåƒæ•¸ \\(\theta\\)ï¼‰æ ¹æ“š state è¼¸å‡º action \\(a_t = P_\theta(s_t)\\)
    * è¨“ç·´æ–¹å¼ï¼šmaximize output of critics
* criticï¼ˆåƒæ•¸ \\(\phi\\)ï¼‰ ä¼°è¨ˆ state-action æœªä¾†çš„å›å ±ï¼ˆexpected discount rewardï¼‰
    * è¨“ç·´æ–¹å¼ï¼šminimise the temporal difference error
* æ›´æ–°æ–¹å¼ï¼š
    * \\( \theta \leftarrow \underset{\theta}{\arg\max} \ Q_\phi(s_t,a_t) \\)
    * \\( \phi \leftarrow \underset{\phi}{\arg\min} \ \left( Q_\phi(s_t,a_t) - r_t  -\gamma Q_\phi(s_{t+1},a_{t+1}) \right)^2 \\)
    
*Meta-Critic MVN + TAEN çš„æ¶æ§‹*

* critc æœ¬ä¾†åªä»¥ state å’Œ action ç‚º inputï¼Œç¾åœ¨åŠ å…¥ task-actor encoding network (TAEN) \\(C_\omega\\)ï¼ˆåƒæ•¸ \\(\omega\\)ï¼‰ ä¾†ç”¢ç”Ÿå­¸ç¿’è»Œè·¡çš„ embedding \\(z_t = C_\omega(L^t_{t-k}) \\)ï¼›\\(L\\) æ˜¯ actor çš„ *learning trace*
    * \\( L^t_{t-k} =[(s_{t-k}, a_{t-k}, r_{t-k}), (s_{t-k+1}, a_{t-k+1}, r_{t-k+1}), \dots, (s_{t-1}, a_{t-1}, r_{t-1})] \\)
        * é€é reward çŸ¥é“é—œæ–¼ task çš„è³‡è¨Šï¼Œé€é state, action çŸ¥é“ actor çš„è³‡è¨Šã€‚
* æ›´æ–°æ–¹å¼ï¼ˆæ¯ \\(M\\) å€‹ task ç‚ºä¸€çµ„ mini-batchï¼Œæ›´æ–°ä¸€æ¬¡ meta-critic åƒæ•¸ï¼‰ï¼š
    * \\( \theta^{(i)} \leftarrow \underset{\theta^{(i)}}{\arg\max} \ Q_\phi(s_t^{(i)}, a_t^{(i)}, z_t^{(i)} ) \quad \forall i \in [1,2,\dots,M] \\)
    * \\( \phi,\omega \leftarrow \underset{\phi,\omega}{\arg\min} \ \sum_{i=1}^M \left( Q_\phi(s_t^{(i)}, a_t^{(i)}, z_t^{(i)} ) - r_t^{(i)}  -\gamma Q_\phi (s_{t+1}^{(i)}, a_{t+1}^{(i)}, z_{t+1}^{(i)}) \right)^2 \\)


### å¾å¼·åŒ–å­¸ç¿’çš„è§’åº¦çœ‹ç›£ç£å¼å­¸ç¿’

* SL å°±åƒåªç©ä¸€æ¬¡çš„ gameï¼š
    1. actor è¼¸å…¥ \\(x\\) è¼¸å‡º prediction \\(\hat{y}\\)
    2. environment çµ¦äºˆä¸€æ¬¡ reward \\(r=-\mathscr{l}(\hat{y}, y)\\) (è² çš„ loss)ï¼Œç„¶å¾ŒéŠæˆ²å°±å®Œæˆäº†ã€‚
* æ›´æ–°æ–¹å¼ï¼š 
    * \\( \theta \leftarrow \underset{\theta}{\arg\max}\, Q_\phi (x, \hat{y}) \\)
    * \\( \phi \leftarrow \underset{\phi}{\arg\min} \left(Q_\phi(x, \hat{y}) - r \right)^2 \\)
        * å…¶ä¸­ \\(\hat{y} = P_\theta(x) \\), and \\( r=-\mathscr{l}(\hat{y}, y) \\)
* æ‰€ä»¥ RL å°ç…§ SLï¼š
    1. state \\( s \rightarrow x \\)ï¼šstate ç›¸ç•¶æ–¼å•é¡Œè³‡è¨Šï¼ˆä¸¦ç„¡å‰ä¸€æ¬¡ actionï¼‰
    2. action \\( a \rightarrow \hat{y} \\)ï¼š prediction å°±æ˜¯ policy network ä½œå‡ºçš„ actionã€‚
    3. reward \\( r \rightarrow -\mathscr{l}(\hat{y}, y) \\)ï¼šç®—å‡ºä¾†çš„ loss è¶Šä½å°±æ˜¯é€™å€‹ action çš„ reward è¶Šå¥½ã€‚
* é€™ç¨®çœ‹æ³•å…¶å¯¦å’Œå‚³çµ± SL æœ‰å¾ˆä¸åŒçš„å‡è¨­ï¼Œå› ç‚ºç¾åœ¨è®Šæˆ actor ä¸¦ä¸çŸ¥é“çœŸå€¼ \\(y\\)ã€ä¹Ÿä¸çŸ¥é“ loss function çš„å½¢å¼ï¼ŒåªçŸ¥é“å¾—åˆ°çš„ reward å¤šå°‘ã€‚Critic çš„è§’è‰²å°±è®Šæˆå­¸ç¿’ä¼°è¨ˆç•¶ä¸‹çš„ reward (å› ç‚º game åªæœ‰ä¸€æ­¥)ï¼Œä¹Ÿå› ç‚ºæœ‰ Critic åœ¨ï¼Œä¸¦ä¸éœ€è¦æ¯ä¸€ç­†è³‡æ–™éƒ½æœ‰çœŸå€¼ã€‚

### é‡å° few-shot å¯¦é©—çš„è¨­è¨ˆ

|     | è¨“ç·´ | è³‡æ–™ | 
|------|------|----|
| (i) Meta-trainging | meta-critics + actors | è¨±å¤š source tasks <br/> è¨±å¤šè³‡æ–™é» / å¤šæ¬¡æ•¸ç’°å¢ƒäº’å‹• |
| (ii) Meta-testing | actors | new tasks <br/> å°‘æ•¸è³‡æ–™é» / å°‘æ¬¡æ•¸çš„ç’°å¢ƒäº’å‹•  | 
| (iii) Final testing | ç„¡ (åƒ…è©•ä¼° actor çš„èƒ½åŠ›) | (same) new tasks  <br/> è¶³å¤ è©•åˆ† |


|  |  SL sine/linear å¯¦é©—<br/>(task ä¸åŒåƒæ•¸) | RL cartpole å¯¦é©— <br/> (task ä¸åŒ pole length)|
|------|----|----|
| (i)  | 10,000 source tasks; å„ 30,000 çš„å€‹è³‡æ–™é» | 1000 tasks; å„ 150,000 äº’å‹• | 
| (ii)  | new tasks; å„ 4, 6 æˆ– 8 å€‹è³‡æ–™é» | ç© 100 games (å€’ä¸‹æˆ–é” 200 æ¬¡å‹•ä½œ) |
| (iii)  | new tasks; å„ 100 è³‡æ–™é»ç®— MSE  | ç© 10 games ç®—å¹³å‡åˆ†æ•¸ |


<small> 
è¨»ï¼šactor éƒ½æ˜¯æ¯å€‹ task å„è‡ªç¨ç«‹ä¸€å€‹ã€‚<br/>
è¨»ï¼šcartpole å‘å·¦æˆ–å‘å³å‹•ä¸€ä¸‹ç®—ä¸€æ¬¡äº’å‹•ã€‚ 
<small/>



## å°è¨˜

1. æ¦‚å¿µä¸Šæ˜¯ç”¨ä¸€å€‹ meta-learner å»å­¸ç‰¹å®šä¸€é¡çš„å•é¡Œçš„å…±é€šçŸ¥è­˜ã€‚å­¸æœƒçš„çŸ¥è­˜å­˜åœ¨ MVN å’Œ TAEN ä¸­ã€‚
2. å’Œ MAMLï¼ˆModel-agnostic meta-learning for fast adaptation of deep networksï¼‰çš„ä½œæ³•æœ‰ç›¸ä¼¼ä¹‹è™•ï¼Œä½†ä½¿ç”¨ RL æ¡†æ¶ä¸‹çš„ crtic å…·æœ‰æ›´ä¸å—é™æ–¼å•é¡Œçš„ prior distribution çš„å¥½è™•ã€‚
3. ç‰¹è‰²ï¼š"Learning to supervise" rather than "learning to synthesize" 

### å¾…ç†è§£

1. å’Œ semi-supervise learning çš„é—œä¿‚ã€‚
2. actor-critic çš„å¯¦ä½œã€‚
3. ç–‘å•ï¼šé€™å€‹ meta-critic å¦‚æœç”¨åœ¨å…¶ä»–ä¸åŒé¡å•é¡Œä¸Šï¼Œæ˜¯ä¸æ˜¯å› ç‚ºå…¶biasè€Œè®Šå¾—æ¯”ç©ºç™½ critic å·®ï¼Ÿ
4. Dirichlet distributionï¼ˆæˆ‘ç•¥é Dependant Multi-arm Bandit å¯¦é©—ï¼‰

{{< katex >}}